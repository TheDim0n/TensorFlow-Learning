# Нейрон (математическая модель нейрона)

**Математическая модель нейрона** (или просто - *нейрон*) состоит из весов $W$, сумматора $\sum$ и функции активации $F$. Нейрон получает входные данные $X$, каждому элементу которого сопоставляется вес $w_i$. Произведения $x_i*w_i$ суммируются и поступают в функцию активации $F$, результат которой является **выходом нейрона**. В самам простом варианте в качестве функции активации выступает тождественная функция.

<center><img src="images\neuron.jpg"/></center>

Математически нейрон можно записать через скалярное произведение векторов $X$ и $W$:
$$ \Large Y = F(\sum_{i=0}^{N} x_{i}w_{i}) = F(\langle X, W \rangle) $$

# Функции активации

1. **Linear (тождественная):** $$ \Large f(x) = x $$
<center><img src="images\linear.png" width=300 height=200/></center>

2. **Sigmoid:** $$\Large f(x) = \sigma(x) = \frac{1}{1 + e^{-x}} $$ 
<center><img src="images\sigmoid.png" width=300 height=200/></center>

3. **Tanh (гиперболический тангенс):** $$\Large f(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $$
<center><img src="images\tanh.png" width=300 height=200/></center>

4. **ReLU:** $$\Large f(x) = \max(x, 0) $$ 
<center><img src="images\relu.png" width=300 height=200 /></center>

5. **SoftMax:** $$ \Large f(x) = \frac{e^{x_{k}}}{\sum_{i=0}^{M} e^{x_{i}}} $$

# Обучение

> Задачи машинного обучения сводятся к задаче минимизации.

Большинство алгоритмов обучения моделей нейронных сетей основываются на *градиентном спуске*. Такие алгоритмы принято называть **оптимизаторами (optimizers)**.

## Градиентный спуск

Метод основан на нахождении градиента функции, который указывает направление её наибольшего возрастания. Модуль градиента равен скорости роста функции.

$$
\Large grad =  \begin{pmatrix}
        \frac{\partial loss}{\partial w_{0}} \\
        \frac{\partial loss}{\partial w_{1}} \\
        \frac{\partial loss}{\partial w_{2}} \\
        ...\\
        \frac{\partial loss}{\partial w_{n}}
    \end{pmatrix}
$$
> $loss$ - функция потерь, $W$ - веса нейронной сети.

Пусть на текущем шаге $i$ имеется набор параметров модели (весов) $W_{i}$. Тогда **правило обновления весов** будет иметь вид:
$$
\Large W_{i+1} = W_{i} - lr*grad(loss, W_{i})
$$
> $lr$ (*learning rate*) - настраиваемый параметр, регулирующий размер шага градиентного спуска.

# функции потерь

Наиболее часто используемыми являются функции потерь **MSE**, **CategoricalСrossentropy (CCE)** и **BinaryСrossentropy (BCE)**:
* $\Large MSE = \frac{1}{N}\sum_{i=0}^{N}(\overline{Y_{i}}-Y_{i})^2$ - *Mean squared error* обычно используется для задач **регрессии**.
* $\Large CCE = -\sum_{i=0}^{N}Y_{i}\log{\overline{Y_{i}}}$ - для многоклассовой классификации.
* $\Large BCE = -Y_{i}\log{\sigma(\overline{Y_{i}})} - (1 - Y_{i})\log{(1-\sigma(\overline{Y_{i}}))}$ - для бинарной классификации.
> $N$ - количество примеров в выборке, $\overline{Y_{i}}$ - значения, предсказанные нейронной сетью, $Y_{i}$ - целевые значения.

# Свёрточные нейронные сети (Convolutional neural network, CNN)

Работа свёрточной нейронной сети обычно интерпретируется как переход от конкретных особенностей изображения к более абстрактным признакам.
Вместе со слоем **свёртки** *(сonvolution)* в нейронных сетях используются слои **паддинга** *(padding)* и **пулинга** *(pooling)*.

## Padding
Это операция, дополняющая матрицу по краям какими-то значениями (по умолчанию - нулём). Операция чаще всего используется перед (или вместе) свёрткой для сохранения начальных размеров матрицы.

## Convolution
Существуют свёртки разных размерностей, рассмотрим пример свёртки для матрицы - **Conv2D**.

На рисунке ниже представлен кусочек цветного (RGB) изображения. Каждому каналу соотвествует своя матрица. В этом примере используется два фильтра размером *size=(3, 3)*, каждый из которых состоит из трех уникальных матриц (ядро, kernel). На выходе получается столько каналов, сколько было задано фильтров.

<center><img src="images\conv.jpg" width=600 height=600 /></center>

Кажде ядро скользит с заданным шагом *stride* по соответствующему ему каналу. На каждом шаге фрагмент канала поэлементно умножается с ядром, результаты суммируются:

<center><img src="images\conv.gif" width=650 height=250 /></center>

Полученные матрицы для каждого канала складываются:

<center><img src="images\conv_2.gif" width=650 height=250 /></center>

Каждое значение итоговой матрицы может быть дополнительно увеличено на *bias*.

> Операция свёртки также является линейным преобразованием, поэтому к ней применяют функции активации.

## Pooling
Слой пулинга представляет собой нелинейное уплотнение карты признаков, при этом группа пикселей (обычно размера 2×2) уплотняется до одного пикселя, проходя нелинейное преобразование. Наиболее эффективным является пулинг с функцией максимума:

<center><img src="images\max_pooling.png" width=450 height=250 /></center>

> Как и слой свёрток, слой пулинга имеет парметры *stride* и *size*.


