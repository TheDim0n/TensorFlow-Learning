{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Нейрон (математическая модель нейрона)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Математическая модель нейрона** (или просто - *нейрон*) состоит из весов $W$, сумматора $\\sum$ и функции активации $F$. Нейрон получает входные данные $X$, каждому элементу которого сопоставляется вес $w_i$. Произведения $x_i*w_i$ суммируются и поступают в функцию активации $F$, результат которой является **выходом нейрона**. В самам простом варианте в качестве функции активации выступает тождественная функция.\r\n",
    "\r\n",
    "![](https://raw.githubusercontent.com/TheDim0n/TensorFlow-Learning/master/images/neuron.jpg)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Математически нейрон можно записать через скалярное произведение векторов $X$ и $W$:\r\n",
    "$$ \\Large Y = F(\\sum_{i=0}^{N} x_{i}w_{i}) = F(\\langle X, W \\rangle) $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Функции активации"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. **Linear (тождественная):** $$ \\Large f(x) = x $$\r\n",
    "![](https://raw.githubusercontent.com/TheDim0n/TensorFlow-Learning/master/images/linear.png)\r\n",
    "\r\n",
    "2. **Sigmoid:** $$\\Large f(x) = \\sigma(x) = \\frac{1}{1 + e^{-x}} $$ \r\n",
    "<img src=\"https://raw.githubusercontent.com/TheDim0n/TensorFlow-Learning/master/images/sigmoid.png\" width=300 height=200/>\r\n",
    "\r\n",
    "3. **Tanh (гиперболический тангенс):** $$\\Large f(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $$\r\n",
    "<center><img src=\"https://raw.githubusercontent.com/TheDim0n/TensorFlow-Learning/master/images/tanh.png\" width=300 height=200/></center>\r\n",
    "\r\n",
    "4. **ReLU:** $$\\Large f(x) = \\max(x, 0) $$ \r\n",
    "<center><img src=\"https://raw.githubusercontent.com/TheDim0n/TensorFlow-Learning/master/images/relu.png\" width=300 height=200 /></center>\r\n",
    "\r\n",
    "5. **SoftMax:** $$ \\Large f(x) = \\frac{e^{x_{k}}}{\\sum_{i=0}^{M} e^{x_{i}}} $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Обучение\r\n",
    "\r\n",
    "> Задачи машинного обучения сводятся к задаче минимизации.\r\n",
    "\r\n",
    "Большинство алгоритмов обучения моделей нейронных сетей основываются на *градиентном спуске*. Такие алгоритмы принято называть **оптимизаторами (optimizers)**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Градиентный спуск\r\n",
    "\r\n",
    "Метод основан на нахождении градиента функции, который указывает направление её наибольшего возрастания. Модуль градиента равен скорости роста функции.\r\n",
    "\r\n",
    "$$\r\n",
    "\\Large grad =  \\begin{pmatrix}\r\n",
    "        \\frac{\\partial loss}{\\partial w_{0}} \\\\\r\n",
    "        \\frac{\\partial loss}{\\partial w_{1}} \\\\\r\n",
    "        \\frac{\\partial loss}{\\partial w_{2}} \\\\\r\n",
    "        ...\\\\\r\n",
    "        \\frac{\\partial loss}{\\partial w_{n}}\r\n",
    "    \\end{pmatrix}\r\n",
    "$$\r\n",
    "> $loss$ - функция потерь, $W$ - веса нейронной сети.\r\n",
    "\r\n",
    "Пусть на текущем шаге $i$ имеется набор параметров модели (весов) $W_{i}$. Тогда **правило обновления весов** будет иметь вид:\r\n",
    "$$\r\n",
    "\\Large W_{i+1} = W_{i} - lr*grad(loss, W_{i})\r\n",
    "$$\r\n",
    "> $lr$ (*learning rate*) - настраиваемый параметр, регулирующий размер шага градиентного спуска."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# функции потерь\r\n",
    "\r\n",
    "Наиболее часто используемыми являются функции потерь **MSE**, **CategoricalСrossentropy (CCE)** и **BinaryСrossentropy (BCE)**:\r\n",
    "* $\\Large MSE = \\frac{1}{N}\\sum_{i=0}^{N}(\\overline{Y_{i}}-Y_{i})^2$ - *Mean squared error* обычно используется для задач **регрессии**.\r\n",
    "* $\\Large CCE = -\\sum_{i=0}^{N}Y_{i}\\log{\\overline{Y_{i}}}$ - для многоклассовой классификации.\r\n",
    "* $\\Large BCE = -Y_{i}\\log{\\sigma(\\overline{Y_{i}})} - (1 - Y_{i})\\log{(1-\\sigma(\\overline{Y_{i}}))}$ - для бинарной классификации.\r\n",
    "> $N$ - количество примеров в выборке, $\\overline{Y_{i}}$ - значения, предсказанные нейронной сетью, $Y_{i}$ - целевые значения."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Свёрточные нейронные сети (Convolutional neural network, CNN)\r\n",
    "\r\n",
    "Работа свёрточной нейронной сети обычно интерпретируется как переход от конкретных особенностей изображения к более абстрактным признакам.\r\n",
    "Вместе со слоем **свёртки** *(сonvolution)* в нейронных сетях используются слои **паддинга** *(padding)* и **пулинга** *(pooling)*.\r\n",
    "\r\n",
    "## Padding\r\n",
    "Это операция, дополняющая матрицу по краям какими-то значениями (по умолчанию - нулём). Операция чаще всего используется перед (или вместе) свёрткой для сохранения начальных размеров матрицы.\r\n",
    "\r\n",
    "## Convolution\r\n",
    "Существуют свёртки разных размерностей, рассмотрим пример свёртки для матрицы - **Conv2D**.\r\n",
    "\r\n",
    "На рисунке ниже представлен кусочек цветного (RGB) изображения. Каждому каналу соотвествует своя матрица. В этом примере используется два фильтра размером *size=(3, 3)*, каждый из которых состоит из трех уникальных матриц (ядро, kernel). На выходе получается столько каналов, сколько было задано фильтров.\r\n",
    "\r\n",
    "<center><img src=\"https://raw.githubusercontent.com/TheDim0n/TensorFlow-Learning/master/images/conv.jpg\" width=600 height=600 /></center>\r\n",
    "\r\n",
    "Кажде ядро скользит с заданным шагом *stride* по соответствующему ему каналу. На каждом шаге фрагмент канала поэлементно умножается с ядром, результаты суммируются:\r\n",
    "\r\n",
    "<center><img src=\"https://raw.githubusercontent.com/TheDim0n/TensorFlow-Learning/master/images/conv.gif\" width=650 height=250 /></center>\r\n",
    "\r\n",
    "Полученные матрицы для каждого канала складываются:\r\n",
    "\r\n",
    "<center><img src=\"https://raw.githubusercontent.com/TheDim0n/TensorFlow-Learning/master/images/conv_2.gif\" width=650 height=250 /></center>\r\n",
    "\r\n",
    "Каждое значение итоговой матрицы может быть дополнительно увеличено на *bias*.\r\n",
    "\r\n",
    "> Операция свёртки также является линейным преобразованием, поэтому к ней применяют функции активации.\r\n",
    "\r\n",
    "## Pooling\r\n",
    "Слой пулинга представляет собой нелинейное уплотнение карты признаков, при этом группа пикселей (обычно размера 2×2) уплотняется до одного пикселя, проходя нелинейное преобразование. Наиболее эффективным является пулинг с функцией максимума:\r\n",
    "\r\n",
    "<center><img src=\"https://raw.githubusercontent.com/TheDim0n/TensorFlow-Learning/master/images/max_pooling.png\" width=450 height=250 /></center>\r\n",
    "\r\n",
    "> Как и слой свёрток, слой пулинга имеет парметры *stride* и *size*."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e8250ac04ae39ec2659d89fea9df3e29398a70e8163a2c098505d395ade54faf"
  },
  "kernelspec": {
   "display_name": "tflearn",
   "language": "python",
   "name": "tflearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}