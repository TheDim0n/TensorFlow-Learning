{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Нормазизация. BatchNormalization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Нормализация данных** - один из способов ускорить обучение нейронной сети. На левом графике показаны сырые данные (для задачи регрессии), которые далеко находятся от начальных весов сети. Чтобы приблизиться к данным, должно быть совершено достаточно большое количество шагов градиентного спуска. если данные будут, нормализованы, тогда количество шагов градиентного спуска заметно уменьшиться (правый график), что позволит ускорить и стабилизировать обучение нейронной сети."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](https://raw.githubusercontent.com/TheDim0n/TensorFlow-Learning/master/images/normalization.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Рассмотрим одну из разновидностей нормализации - **BatchNormalization** - нормализация по батчу (пакету данных). Пусть $ B = \\{x_{1}, x_{2}, ..., x_{N}\\}$ - батч, тогда нормализация по батчу:\n",
    "$$ \\Large BN_{\\alpha, \\beta}({X}_{i}) = \\alpha\\hat{X} + \\beta,$$\n",
    "где:\n",
    "$ \\Large \\hat{X} = \\frac{x_{i}-\\mu_{B}}{\\sqrt{\\sigma^{2}_{B}+\\epsilon}} $ $\\Large-$ нормализация, <br><br>\n",
    "$ \\Large \\sigma^{2}_{B} = \\frac{1}{N} \\sum _{i=1} ^{N} (x_{i}-\\mu_{B})^{2} $ $\\Large-$ дисперсия батча, <br><br>\n",
    "$ \\Large \\mu_{B} = \\frac{1}{N} \\sum _{i=1} ^{N} x_{i} $ $\\Large-$ математическое ожидание батча, <br><br>\n",
    "$ \\Large \\alpha, \\beta $ $\\Large-$ обучаемые параметры слоя. <br><br>\n",
    "Слой батч-нормализации можно применять не только ко входному слою (к сырым данным), но и к последующим слоям нейронной сети (признакам). Например:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "tf.keras.Sequential([\r\n",
    "    tf.keras.layers.InputLayer((28, 28, 1)),\r\n",
    "    tf.keras.layers.BatchNormalization(),\r\n",
    "    tf.keras.layers.Conv2D(64, 3, 1, padding='same', activation='relu'),\r\n",
    "    tf.keras.layers.MaxPool2D(),\r\n",
    "    tf.keras.layers.BatchNormalization(),\r\n",
    "    tf.keras.layers.Conv2D(128, 3, 1, padding='same', activation='relu'),\r\n",
    "    tf.keras.layers.MaxPool2D(),\r\n",
    "    tf.keras.layers.Conv2D(32, 3, 1, padding='same', activation='relu'),\r\n",
    "    tf.keras.layers.MaxPool2D(),\r\n",
    "    tf.keras.layers.BatchNormalization(),\r\n",
    "    tf.keras.layers.Flatten(),\r\n",
    "    tf.keras.layers.Dense(512),\r\n",
    "    tf.keras.layers.BatchNormalization(),\r\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\r\n",
    "], name='model').summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_8 (Batch (None, 28, 28, 1)         4         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 14, 14, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 32)          36896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 3, 3, 32)          128       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               147968    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 266,926\n",
      "Trainable params: 265,708\n",
      "Non-trainable params: 1,218\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Регуляризация. L1, L2 и Dropout\n",
    "**Переобучение** модели нейронной сети - это ситуация, когда модель \"выучила\" тренировочные данные, однако закономерность в них не выявила. В таком случае метрики на тренировочном датасете будут высокими, а на тестовом - низкими.\n",
    "Основные причины переобучения:\n",
    "- слишком много параметров нейронной сети;\n",
    "- мало данных (Imbalanced Data).\n",
    "\n",
    "С первой причиной борются методы регуляризации. Рассмотрим их подробнее.\n",
    "\n",
    "## L1-регуляризация\n",
    "Допустим, мы обучаем нейронную сеть на функции ошибки **MSE** (может быть и любая другая):\n",
    "$$ \\Large MSE = \\frac{1}{N} \\sum _{i=1} ^{N} (y_{i}-\\hat{y})^{2}$$\n",
    "Давайте добавим к ней сумму модулей весов сети. Мы получили формулу L1-регуляризации (её также называют LASSO-регуляризацией):\n",
    "$$ \\Large Loss = MSE + \\alpha \\sum _{i=1} ^{M} |w_{i}|,$$\n",
    "где: <br><br>\n",
    "$ \\Large M $ $\\Large-$ количество обучаемых параметров модели, <br><br>\n",
    "$ \\Large w_{i} $ $\\Large-$ $i$-ый вес модели. <br><br>\n",
    "## L2-регуляризация\n",
    "На этот раз добавим к функции потерь сумму квадратов весов. Мы получили формулу L2-регуляризации (её также называют регуляризацией Тихонова):\n",
    "$$ \\Large Loss = MSE + \\alpha \\sum _{i=1} ^{M} w_{i}^2,$$\n",
    "где: <br><br>\n",
    "$ \\Large M $ $\\Large-$ количество обучаемых параметров модели, <br><br>\n",
    "$ \\Large w_{i} $ $\\Large-$ $i$-ый вес модели. <br><br>\n",
    "В обоих методах $\\Large \\alpha$ $\\Large-$ настраиваемый параметр.\n",
    "\n",
    "## Dropout\n",
    "Dropout, как и регуляризация, не позволяет весам нейронной сети принимать слишком большие (или слишком маленькие) значения. Метод заключается в обнулении определнного процента весов слоя (обычно это 30-50%) на время обучения - фактически урезается кол-во параметров слоя и ,как следствие, всей сети. На каждом шаге градиентного спуска обнуляемые веса выбираются случайным образом.\n",
    "\n",
    "Пример реализации в Keras вышеописанных методов:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "tf.keras.Sequential([\r\n",
    "    tf.keras.layers.InputLayer((100,)),\r\n",
    "    tf.keras.layers.Dense(1024, kernel_regularizer=tf.keras.regularizers.L2(0.1)),\r\n",
    "    tf.keras.layers.Dropout(0.5), # обнуляем 50% весов ТОЛЬКО ПРЕДЫДУЩЕГО СЛОЯ\r\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\r\n",
    "], name='model').summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 1024)              103424    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 113,674\n",
      "Trainable params: 113,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Документация по регуляризации и Dropout:<br>\n",
    "L1 - https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L1 <br>\n",
    "L2 - https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2 <br>\n",
    "L1L2- https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L1L2<br>\n",
    "Dropout - https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout <br>"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}